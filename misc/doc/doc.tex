
\documentclass[a4paper,oneside,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xspace}
\usepackage[french]{babel}
\usepackage{multicol}
\usepackage{geometry}

% #1 name of picture
% #2 caption
% #3 width
\newcommand\appendixpicture[3]{
	\begin{figure}[h]
	\centering
	\includegraphics[width=#3cm]{content/#1.png}
	\caption{#2}
	\label{#1}
	\end{figure}
}

\geometry{hmargin=2cm,vmargin=1.5cm}

\title{Twitter \& Humeur : Documentation\\
\small{Système distribué pour le traitement de données}
}

\author{Jordan DE GEA - Guillaume THIOLLIERE - Vincent TAVERNIER - Pierre HEINISCH\\
William DUCLOT - Mathieu STOFFEL - Joseph PEREZ - Imane Rachdi - Salim ABOUBACAR}

\begin{document}

\maketitle

\tableofcontents

\pagebreak
\section{Sujet - "Twitter \& Humeur"}

L'objectif de ce projet est d'estimer l'\textbf{humeur} des gens dans différentes régions. Nous récupérons des flux twitters sur différentes régions. Sur chaque tweet, nous attribuons une appréciation. Puis nous stockons chaque tweet traité et son appreciation dans la base. 

\pagebreak
\section{Composants}

\subsection{Kafka}

\textbf{Kafka} est un système de messagerie distribué. Il joue le rôle de 
{broker} pour des flux de données : des \textbf{producteurs} envoient des flux de données à Kafka, qui va les stocker et permettre à des \textbf{consommateurs} de traiter ces flux.

Chaque flux peut être partitionné, permettant à plusieurs consommateurs de travailler sur le même flux en parallèle. Une partition est une suite de messages ordonnés et immutables, chaque message ayant un identifiant qui lui est affecté.

\textbf{Kafka} fonctionne en cluster, permettant de répliquer les partitions pour être tolérant aux fautes, d'automatiquement redistribuer les consommateurs en cas de faute et d'être très facilement horizontalement scalable.

Dans notre cas, Kafka sera utilisé pour s'abonner aux flux Twitter et météo, et partitionner ces flux par région.

\textbf{Kafka} est tolérant aux pannes franches, du fait de la réplication des partitions sur les différents serveurs (réplication configurable). Cette tolérance est valable si on considère que les communications entre producteur et cluster Kafka sont fiables. En effet la réplication se fait au sein du cluster Kafka, les serveurs de réplication (\textit{followers}) pour une partition donnée copiant la partition depuis le serveur référent (\textit{leader}) pour cette partition. Dés lors si le producteur ne parvient pas à contacter le serveur référent, le message ne sera pas renvoyé et sera perdu. 

\textbf{Utilisé par :} Netflix, PayPal, Uber...


\subsection{Flink}

\textbf{Apache Flink} est un framework de traitement temps-réel. Il permet donc de traiter des données arrivant en temps-réel, plutôt que par \textit{batch}, et donc d'avoir un temps de latence extrêmement court.

En utilisant \textbf{Flink} pour traiter les flux fournis par Kafka, nous conservons l'aspect temps-réel qui fait la particularité de Twitter.

\textbf{Utilisé par :} Bouygues Telecom, Alibaba, Amadeus, ATOS...

\subsection{HBase}

HBase est une base de données distribuée non-relationnelle. Cette technologie permet de stocker de larges quantités de données, et est très efficace pour les applications ayant un haut débit de données.

Hbase gère la réplication au sein du cluster, le sharding et le équilibrage de charge. Le requêtage est extrêmement rapide et des filtres peuvent être appliqués.

\textbf{Utilisé par :} Adobe, Airbnb, Facebook Messenger, Netflix...

\textbf{HBase} assure une cohérence stricte des écritures et lectures, c'est à dire qu'une lecture renvoie toujours le résultat de la dernière écriture effectuée.
HBase gère de manière automatique la réplication au sein du cluster ainsi que le basculement en cas de panne.

\subsection{Zeppelin}

\textbf{Zeppelin} fournit une interface web de visualisation de données. Son principal intérêt est d'être capable d'analyser et mettre en forme de grandes quantités de données, et de s'intégrer très bien aux autres technologies (faisant partie de l'écosystème Apache).

Cet outil fournit de nombreux graphes pour la visualisation de données, permettant de rapidement et facilement travailler sur de gros volumes.

Zeppelin, en tant que simple outil de visualisation, ne garantit en rien la tolérance aux fautes. Cependant, Zeppelin intervient en bout de chaîne et son interruption n'a aucune incidence sur le fonctionnement du reste des composants du système. 

\pagebreak
\section{Architecture}

\subsection{Producer : Flink}

Le \textbf{Producer} s'inscrit sur un flux twitter pour récupérer les tweets sur les régions désirées. A chaque réception de tweets, le produceur distribue les tweets sur \textbf{Kafka}

Annexe \ref{flinkkafka}

\subsection{Distributeur : Kafka}

Notre \textbf{Kafka} est découpé en ville, chaque \textit{topic} correspond à une ville. Kafka stocke message par message les tweets pour chaque \textit{topic}. 

Annexe \ref{kafkaflink}

\subsection{Traitement : Flink}

Nos \textbf{Flink} de traitement sont découpés en ville. Il récupère message par message, les tweets de leur ville dans le \textit{topic} associé dans \textbf{Kafka}. Il traite chaque tweet afin d'attribuer une appréciation au tweet. 

Annexe \ref{flinkhbase}

\subsection{Base de données : HBase}

Nous stockons chaque tweet dans une table correspondant à sa ville. 

Annexe \ref{hbasezeppelin}

\subsection{Visualisation : Zeppelin}

Nous visualisons nos données grâce à \textbf{Zeppelin}

\pagebreak
\section{Comportement}

Nous considérons 3 serveurs. Nous souhaitons garantir le fonctionnement du service en tolérant deux fautes. 

- Lorsque 3 machines sont en vie, alors le service fonctionne correctement.
- Lorsque 2 machines sont en vie, alors le service fonctionne correctement et cherche à remettre en place la machine en faute.
- Lorsque 1 machine est en vie, alors le service fonctionne correctement et à remettre en place les machines en faute. 

Nous éxécutons le projet sur 3 villes : Paris, London, NYC

\subsection{Configuration}

\subsubsection{Kafka}

\begin{itemize}
\item Installé sur toutes les machines. 
\item Un détecteur de faute se chargera de détecter les machines fautives.
\item Un correcteur de faute cherchera à redémarrer la ou les machines fautives.
\end{itemize}

\subsubsection{Flink}

\begin{itemize}
\item Installé sur 3 machines. 
\item Un détecteur de faute se chargera de répartir les différentes configurations de Flink sur les différents serveurs en vie
\end{itemize}

\subsubsection{Hbase}

\begin{itemize}
\item Installé sur 3 machines. 
\end{itemize}	

\subsubsection{Zeppelin}

\begin{itemize}
\item Installé sur 3 machines. 
\item Un détecteur de faute cherchera à redémarrer la ou les machines fautives 
\end{itemize}

\subsubsection{Vues logiques, physiques et comportements}

Voir annexes : \ref{vuelogique}, \ref{archioptimal}, \ref{archireel}, \ref{archicrashservice}, \ref{archicrashmachine}, \ref{archicomeback}



\pagebreak
\section{Configurations et Aides}

\subsection{Configuration \& Aide pour Flink Producer}

\subsubsection{Pour commencer}

\textbf{Information}

\begin{verbatim}
 # Aller dans le dossier de glink
cd <flink_directory>

# Lancer une tache
/bin/flink run <Task>.jar <params>
\end{verbatim}

\subsubsection{Information}

Les sources des tâches sont dans le dossier `source/apps/FlinkProducer`. 

\begin{itemize}
\item FakeTwitter : Envoi periodiquement un tweet à kafka, dans une table définie aléatoirement. 
\end{itemize} 


\subsubsection{Objectifs}

Récupérer tous les tweets des lieux où nous souhaitons effectuer nos calculs. Sépare les tweets par lieu et les envoi sur Kafka sur le bon topic. 


\subsubsection{Paramètres}

\begin{itemize}
\item \verb!number of tweet par second! 
\item \verb!bootstrap server!
\end{itemize}

\begin{verbatim}
# Run example
bin/flink run FakeTwitter.jar 10 worker1:9092,worker2:9092,worker3:9092
\end{verbatim}

\subsubsection{Autres}

Lorsque des données sont perdues, nous ne cherchons pas à les récupérer. Si les calculs échouent de même. N'oublions pas l'esprit du Streaming. 
Si Kafka n'est pas accessible à un moment, on ne cherche pas a envoyer de nouveau les tweets, on les oublie. 



\pagebreak
\subsection{Configuration d'Apache Kafka}

\subsubsection{Introduction concernant Apache Kafka}

\textbf{Principe de fonctionnement}

Apache Kafka est un système de \textbf{gestion de messages applicatifs}. 

En quelques mots, lorsque des informations doivent être stockées et 
communiquées de manière structurée entre plusieurs applications 
potentiellement hétérogènes, Apache Kafka est une des solutions 
envisageables. Ses caractéristiques intrinsèques permettent à Apache 
Kafka d'atteindre des performances, notamment en termes d'écriture
sur disque des messages à stocker. C'est pour cela qu'Apache Kafka 
occupe une position dominante lorsqu'il s'agit de \textbf{streaming} de
messages applicatifs.

Apache Kafka met à disposition des API permettant à toute application 
de se connecter à lui, dont notamment :
\begin{itemize}
\item une interface pour la production de message qui seront stockés 
et mis à disposition par Apache Kafka, on parle d'interface 
\textbf{Producer} ;
\item une interface pour accéder aux messages stockés par Kafka, on parle d'interface **Consumer**.
\end{itemize}

L'architecture interne d'Apache Kafka repose sur le principe de 
\textbf{publication/abonnement}. Pour le dire plus simplement, Apache Kafka
offre la possibilité de structurer les messages stockés sous forme
de topics (généralement, la ségrégation des messages entre les
différents topics repose sur des considérations thématiques).
Les différents producteurs de messages peuvent alors décider dans
quel topic ils souhaitent publier leurs messages. 
De même, les consommateurs de messages peuvent choisir les topics
auxquels ils s'abonnent. Les messages étant stockés sur les 
périphériques de stockage de données physiques des machines hébergeant
Apache Kafka, plusieurs consommateurs différents peuvent s'abonner au
même topic, et chacun pour lire l'ensemble des messages y figurant
exactement une fois.

De plus, Apache Kafka est pensé pour être une application distribuée.
Ainsi, les fonctionnalités de réplication des messages entre 
différents serveurs (on réplique des \textit{partitions}, une partition 
pouvant contenir plusieurs \textit{topics}) sont nativement présentes.
Cela permet de mettre en place une politique de tolérance aux fautes
assez aisément.

\textbf{Dépendance envers Apache Zookeeper}

Afin d'assurer les propriétés de réplication et de tolérance aux 
pannes explicitées ci-dessus, Apache Kafka a recours aux services
proposés par Apache Zookeeper.

En conséquence, avant d'instancier un cluster Apache Kafka, il faut
s'assurer d'avoir mis à disposition de ce dernier un quorum Zookeeper.

\textbf{Prendre Kafka en main}

Afin de clôturer cette présentation rapide , voici quelques 
manipulations basiques pour établir un premier contact pratique 
avec Apache Kafka :
\begin{verbatim}

# Décompresser et se rendre dans le dossier d'Apache Kafka.
tar -xzf kafka_2.11-0.10.1.0.tgz
cd kafka_2.11-0.10.1.0

# Démarrer un daemon Apache Zookeeper.
bin/zookeeper-server-start.sh config/zookeeper.properties

# Démarrer un serveur Apache Kafka.
bin/kafka-server-start.sh config/server.properties

# Créer un topic dans une partition unique sans réplication.
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 
	--partitions 1 --topic test
	
# Lister les topics existants.
bin/kafka-topics.sh --list --zookeeper localhost:2181

# Publier des messages.
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message

# Lire lesdits messages.
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test 
	--from-beginning
\end{verbatim}

\subsubsection{Gestion manuelle du cycle de vie}

Si l'on souhaite gérer manuellement le cycle de vie d'Apache Kafka 
(c'est-à-dire sans avoir recours à \textit{ServiceWatcher}), il est 
recommandé d'utiliser \textit{systemd} :

\begin{verbatim}
# Monitorer l'état d'Apache Kafka.
sudo systemctl status kafka
# Démarrer le service associé à Apache Kafka.
sudo systemctl start kafka
# Stopper le service associé à Apache Kafka.
sudo systemctl stop kafka
\end{verbatim}

De plus, \textit{systemd} essayera de redémarrer Apache Kafka en cas d'erreur
inopinée.

\subsubsection{Architecture externe du cluster Apache Kafka}

Dans le but d'assurer une tolérance aux fautes de l'ordre de 2 pannes, nous
avons opté pour un cluster de trois serveurs Apache Kafka (et un quorum Zookeeper
sous-jacent de même taille), avec un facteur de réplication de 3 pour les partitions.

\subsubsection{Architecture interne du cluster Apache Kafka}

Afin de profiter des propriétés de réplication offertes par Apache Kafka tout
en assurant une répartition viable de la charge de données, nous avons
décidé d'instancier une partition (comptant donc un unique topic) par ville.

En effet, la réplication proposée par Apache Kafka est une réplication au
niveau des partitions. Ainsi, dans l'hypothèse où nous aurions opté pour un
topic par ville, l'ensemble des données aurait été contenu dans une seule
partition. Cela explique notre choix d'instancier une partition par ville.

Pour résumer l'architecture interne adoptée pour le cluster Apache Kafka :
\begin{itemize}
\item Partition \textbf{\_nycconsumer\_} :
   Topic \textbf{\_tweets\_}.
\item Partition \textbf{\_londonconsumer\_} :
  Topic \textbf{\_tweets\_}.
\item Partition \textbf{\_parisconsumer\_} :
  Topic \textbf{\_tweets\_}.
\end{itemize}
  
  

\pagebreak
\subsection{Configuration \& Aide pour Flink Processing}

\subsubsection{Information}

Les sources des tâches sont dans le dossier \verb!source/apps/FlinkProcess!. 

\begin{itemize}
\item KafkaToConsole : Récupère les tweets de Kafka et les écrits dans la console
\item KafkaToHBase : Récupère les tweets de Kafka et les envoi à HBase
\end{itemize}

\subsubsection{Objectifs}

A chaque tweet, nous estimons son humeur et ecrivons le resultat dans la base de données. 

Les tweets sont continuellement récupérés de Kafka, estimés et enregistrés dans HBase. 

\subsubsection{Paramètres}

\begin{itemize}
\item \verb!--port <flink port>! 
\item \verb!--topic <place>!
\item \verb!--bootstrap.servers <bootstrap servers>!
\item \verb!--zookeeper.connect <zookeeper machine:port>!
\item \verb!--group.id <consumer group>!
\item \verb!--hbasetable <topic name>!
\item \verb!--hbasequorum <hbase quorum>!
\item \verb!--hbaseport <port of hbase quorum>!
\end{itemize}

\begin{verbatim}
# Run example
bin/flink run KafkaConsoleBridge.jar \
	--port 9000 \
	--topic paris \
	--bootstrap.servers worker1:9092,worker2:9092,worker3:9092 \
	--zookeeper.connect localhost:2181 \
	--group.id parisconsumer \
	--hbasetable paris_tweets \
	--hbasequorum worker1,worker2,worker3 \
	--hbaseport 2181
\end{verbatim}

\subsubsection{Autres}

Lorsque des données sont perdues, nous ne cherchons pas à les récupérer. Si les calculs échouent de même. N'oublions pas l'esprit du Streaming. 

\pagebreak
\subsection{Configuration \& Aide pour HBase}


\subsubsection{Pour commencer avec HBase}

\textbf{Information}

HBase enregistre les données sous forme de ligne texte. Chaque ligne correspond à un couple \textit{<row, column>}. 
Par exemple, \\
\verb!row1      column=col1:a, timestamp=1481392727887, value=value2!

\begin{itemize}
\item \textbf{row1} est l'identifiant de la ligne, HBase store les données par leur identifiant de ligne en respectant l'ordre lexicographique. 
\item \textbf{col1} est le nom de la colonne
\item \textbf{a} est un \textit{qualifier}. Une colonne peut contenir un nombre illimité de \textit{qualifier}. Nous ne l'utiliserons pas dans ce projet. 
\item \textbf{timestamp} est l'heure à laquelle le donnée a été ecrite. 
\item \textbf{value2} est la valeur dans la colonne. 
\end{itemize}

\textbf{Commandes}

\begin{verbatim}
# Creer la table "test" avec deux colonnes "col1", "col2" 
create 'test', 'col1', 'col2'

# Inserer une ligne avec l'identifiant <row1>
# La commande <Put> entre colonne par colonne, remplissons les deux colonnes
put 'test', 'row1', 'col1', 'value11'
put 'test', 'row1', 'col2', 'value12'

# Ecrire d'autres lignes pour la demonstration
put 'test', 'row2', 'col1', 'value21'
put 'test', 'row2', 'col2', 'value22'
put 'test', 'row3', 'col1', 'value31'
put 'test', 'row3', 'col2', 'value32'
put 'test', 'row4', 'col1', 'value41'
put 'test', 'row4', 'col2', 'value42'

# Afficher tout le contenu
scan 'test'

# Afficher depuis <row2> jusqu'à <row4> exclu
scan 'test', {STARTROW => 'row2', ENDROW => 'row4'}
scan 'test', {STARTROW => 'row2', STOPROW => 'row4' }

# HBase cherche par prefix. Les deux commandes suivantes fournissent le meme resultat.
scan 'test', {STARTROW => 'row'}
scan 'test', {STARTROW => 'row1'}

# Afficher avec un filtre sur la colonne
scan 'test', { STARTROW=>'row2', STOPROW=>'row4', COLUMNS=>'col1'}

# Afficher les colonnes pour la ligne *row*
get 'test', 'row1'

# Mettre a jour une valeur
put 'test', 'row4', 'col2', 'newvalue42'

# Vider une table
truncate 'test'

# Desactiver une table
disable 'test'

# Supprimer une table
drop 'test'

\end{verbatim}

\subsection{Configuration \& Aide pour Zeppelin}

\subsubsection{Pour commencer}

\textbf{Information}

Apache Zeppelin est un système de visualisation de données au travers d'une 
interface web. 
Il permet d'accéder et de mettre en forme de gros volumes de données,
de façon visuelle et interactive, afin de permettre leur analyse.

Grâce à un système de plugins et d'interpréteurs, Apache Zeppelin peut 
s'interfacer avec de nombreux systèmes et langages, ici notamment avec 
Apache HBase.

\textbf{Commands}

Une fois Apache Zeppelin lancé sur un serveur, il suffit de se connecter 
au serveur en question sur le port 8080 via un navigateur web pour y accéder.

Apache Zeppelin fonctionne grâce à des notes, permettant d'écrire les 
requêtes souhaitées aux différents systèmes avec lesquels Zeppelin
s'interface.

Pour créer un nouveau Notebook, cliquez sur \textit{Notebook} -> \textit{Create new note}.
Si l'intrepreteur du système ou langage que vous désirez utiliser est installé
et Apache Zeppelin correctement configuré, il suffira d'écrire en début de note
\%\textit{nom\_du\_système\_ou\_langage} puis d'écrire les commandes du système en question.
Par exemple pour Apache HBase, la note suivante permet de lister les tables :  
\begin{verbatim}
%hbase  
list
\end{verbatim}

La configuration de Apache Zeppelin dépend de l'interpréteur que l'on veut utiliser,
mais s'effectuera en modifiant les fichiers \verb!conf/zeppelin-env.sh! et \verb!conf/zeppelin-site.xml!
du dossier d'installation de Apache Zeppelin.

\subsubsection{Utilisation manuelle de Zeppelin}

Si l'on souhaite gérer manuellement le cycle de vie d'Apache Zeppelin 
(c'est-à-dire sans avoir recours à \textit{ServiceWatcher}), il est 
recommandé d'utiliser \textit{systemd} :

\begin{verbatim}
# Monitorer l'état d'Apache Zeppelin.
sudo systemctl status zeppelin
# Démarrer le service associé à Apache Zeppelin.
sudo systemctl start zeppelin
# Stopper le service associé à Apache Zeppelin.
sudo systemctl stop zeppelin
\end{verbatim}

De plus, \textit{systemd} essayera de redémarrer Apache Zeppelin en cas d'erreur
inopinée.

\subsubsection{Zeppelin via HAProxy}

Dans le cadre de ce projet, la connection à Zeppelin peut s'effectuer directement
sur un des serveurs où HAProxy est lancé, sur le port 80, et la redirection vers
Apache Zeppelin s'effectuera automatiquement.


\subsubsection{Information}

Pour découper l'enregistrement des tweets, on enregistre chaque tweet dans le lieu correspondant. C'est à dire, chaque lieu à sa propre table. De cette manière, on evite l'utilisation des filters. Le nom de la table est \textbf{<Place>\_Tweets}

\subsubsection{Structure des tables}

Table \textbf{<Place>\_Tweet}
\begin{itemize}
\item \textbf{\textit{RowIdentifier}} - \textit{timestamp\_UniqID} : identifiant unique.
\item \textbf{user} : propriétés de l'utilisateur auteur du tweet
\item \textbf{tweet} : propriétés du tweet
\item \textbf{feeling} : estimation de l'humeur du tweet
\end{itemize}

\pagebreak

\section{Pour commencer}

\subsection{Outils}

Environnement de déploiement : \textbf{Vagrant}

Outil de deploiement : \textbf{Rake}

\subsection{Lancer en developpement local}

\begin{verbatim}
export RAKE_ENV=development
rake vagrant:up
./deploy.sh
\end{verbatim}

\subsection{Lancer en production}

Le document hosts.yml contient les informations de connexion aux machines de production. 

\begin{verbatim}
export RAKE_ENV=production
./deploy.sh
\end{verbatim}

\pagebreak
\section{Utilisation de Rake pour les tâches de maintenance}

\subsection{Installation}

\begin{verbatim}
# Installation de Bundler (once)
gem install bundler

# Installation des dépendances
# En cas d'incompatibilité avec Gemfile.lock sur le dépôt : bundle update
bundle
\end{verbatim}

\subsection{Environnement}

\begin{verbatim}
# Utilisation de l'environnement de développement
export RAKE_ENV=development
# Ne pas oublier de démarrer les machines virtuelles pour l'environnement 
# de développement
rake vagrant:up

# Utilisation de l'environnement de production
export RAKE_ENV=production
\end{verbatim}

\subsection{Affichage de l'aide}

\begin{verbatim}
# Listing des tâches Rake avec leur description
rake -T
\end{verbatim}

\subsection{Deploiement}

\begin{verbatim}
# Déploiement et provisioning
rake deploy

# Uniquement sur deux serveurs (définis dans hosts.yml)
rake deploy[server-2;server-3]

# Connexion SSH
rake ssh server-1
\end{verbatim}

\subsection{Services}

En l'absence de services spécifiés pour ces commandes, l'ensemble de services
concerné est celui défini dans la section \textit{services} du fichier de configuration
courant. Ces commandes (à l'exception de `services:watcher`) correspondent
à l'appel de `systemctl` sur les machines cibles.

\begin{verbatim}
# Démarrage des services
rake services:start

# Statut des services
rake services:status

# Arrêt des services
rake services:stop

# Kill des services
rake services:kill

# Kill tous les services de <server1> et <server2>
rake services:kill[<server1>;<server2>]

# Kill les services <service1> et <service2> de <server1> et <server2>
rake services:kill[<server1>;<server2>,<service1>;<service2>]

# Kill les services <service1> et <service2> sur tous les serveurs
rake services:kill[,<service1>;<service2>]

# Activation au démarrage (enable)
rake services:enable
\end{verbatim}

\subsection{Execution OneShot de commande}

\begin{verbatim}
# Lancer la commande <commande> (défini dans le yaml config)
rake run:<commande>[<server1>,<server2>]
\end{verbatim}


\subsection{Tests automatisés}

Le fonctionnement de l'infrastructure actuelle (selon \verb!$RAKE_ENV!) peut être
validé par l'exécution de tests automatisés, tels que définis dans le dossier
\verb!features/!.

\begin{verbatim}
# Exécution de tous les tests
rake features

# Exécution des tests de déploiement
rake features:deployment

# Exécution des tests du composant service_watcher
rake features:service_watcher

# Exécution des tests de contrôle des services systemd
rake features:services
\end{verbatim}

\pagebreak
\section{Environnement de test Vagrant}

Le dossier vagrant contient :
\begin{itemize}
	\item  \verb!Vagrantfile! : définition de machines virtuelles de test
	\item  \verb!vagrant-hosts.yml! : configuration des différentes machines de test
\end{itemize}

Pour utiliser cet environnement de test :

\begin{itemize}
\item Installer [Vagrant](https://www.vagrantup.com/).
\item Installer [VirtualBox](https://www.virtualbox.org/).
\item Installer le plugin "vagrant-hostmanager" :
\end{itemize}

\begin{verbatim}
vagrant plugin install vagrant-hostmanager
\end{verbatim}

\begin{itemize}
\item Suivre les instructions de \\
	\texttt{https://github.com/devopsgroup-io/vagrant-hostmanager\#passwordless-sudo} pour éviter la demande de mot de passe sudo à chaque démarrage des machines virtuelles.
\end{itemize}
Une fois que tout est installé, les commandes suivantes peuvent être utilisées :

\begin{verbatim}
# Démarrage des machines définies dans le Vagrantfile
# La première fois, les images vont être téléchargées et configurées. Cela prend du temps.
# Les fois suivantes les machines installées seront reprises dans leur état actuel.
vagrant up

# Connexion SSH à une des machines
vagrant ssh worker1

# Recréer les machines suite à une modification du Vagrantfile
vagrant reload

# Mise en pause des machines
vagrant suspend

# Arrêt des machines (shutdown)
vagrant halt

# Destruction des machines (pour réinstallation propre)
vagrant destroy
\end{verbatim}

Si \_vagrant-hostmanager\_ est configuré correctement, les machines peuvent être contactées en utilisant les noms \verb!worker1!, \verb!worker2! etc. plutôt que leurs adresses IP.

Le réseau privé utilisé pour les machines Vagrant est 10.20.1.0/24. Par défaut l'adresse de la machine hôte est 10.20.1.1.

\subsection{Troubleshooting}

\verb!#vagrant plugin install vagrant-hostmanager échoue!

sous debian il peut être nécessaire d'installer le paquet ruby-dev.

\appendix

\section{Images}

\appendixpicture{vuelogique}{Vue Logique}{15}
\appendixpicture{archioptimal}{Attribution Optimale}{15}
\appendixpicture{archireel}{Attribution Réel}{15}
\appendixpicture{archicrashservice}{Crash d'un service}{15}
\appendixpicture{archicrashmachine}{Crash d'une machine}{15}
\appendixpicture{archicomeback}{Restauration d'une machine}{15}

\appendixpicture{flinkkafka}{De Flink Producer à Kafka}{15}
\appendixpicture{kafkaflink}{De Kafka à Flink}{15}
\appendixpicture{flinkhbase}{De Flink à HBase}{15}
\appendixpicture{hbasezeppelin}{de HBase à Zeppelin}{15}

\end{document}
